{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data and Joining Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find 2 datasets \n",
    "import pandas as pd\n",
    "data = pd.read_csv('2024_medalists_all.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets\n",
    "\n",
    "In python we will load our datasets with different commands based on the format of our dataset.\n",
    "\n",
    "**CSV**  \n",
    "\n",
    "This is the most common format of dataset we will come across. CSV stands for comma-separated values. In pandas we can use the following command to load the dataset:  \n",
    "`var_name = pd.load_csv('filepath')`  \n",
    "This will set the var_name to the data, and will load the data from the file path (this can be an absolute or relative path (relative means from the folder your file is in, absolute would be from the root directory of your operating system))\n",
    "\n",
    "**XLSX (Microsoft Excel)**  \n",
    "\n",
    "This is another common format of dataset. In pandas we can use the following command to load the dataset:\n",
    "`var_name = pd.read_excel('filepath')`  \n",
    "There are lots of powerful optional arguments that you can refer to the documentation on. The only one that could be generally useful is the sheet_name parameter:\n",
    "\n",
    "Unlike R, Python is a 0 index language, so it is defaulted at 0 (so the first sheet). If you need to pull data from a different sheet the simplest way to do this is to specify the sheet number or name in the sheet_name parameter. \n",
    "\n",
    "Imagine I have a excel sheet with 2 sheets, original_data, and corrected_data. If I use the command with just the filepath I will pull the data from the original_data sheet. If I want the corrected_data, I have 2 options: \n",
    "\n",
    "- Use the index:\n",
    "\n",
    "`var_name = pd.read_excel('filepath', sheet_name=1)`\n",
    "\n",
    "- Use the sheet name:\n",
    "\n",
    "`var_name = pd.read_excel('filepath', sheet_name='corrected_data')`\n",
    "\n",
    "*Note that for excel files you must have the `openpyxl` library installed as well*\n",
    "\n",
    "**TXT**\n",
    "\n",
    "These are much less common but some old census files I have seen usep pipe-delimited text files. In this case you will have to use a different command and declare the delimiter if it is anything but tab delimited. \n",
    "\n",
    "`var_name = pd.read_table('filepath', sep = '|')`\n",
    "\n",
    "^ This example is for pipe-delimited text file parsing. \n",
    "\n",
    "The data would look something like this:\n",
    "\n",
    "> name | age | income   \n",
    "> Mark | 25 | 0\n",
    "\n",
    "\n",
    "For any other formats of data (JSON, XML, SHP, etc) reach out to your project leads or me for help (often ChatGPT can help)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Cleaning Data\n",
    "\n",
    "Cleaning data is often an important first step, data is rarely clean and nicely formatted. This can range from having to change datatypes to having to deal with null values, or oddly inputted null values for example using a string with \"No Value Found\". When starting out with a new dataset there are a few helpful commands we can use to see different pieces of information about our data set. \n",
    "\n",
    "To get a peak at our data set we can use the `head()` command. \n",
    "``` python\n",
    "display(data.head())\n",
    "```\n",
    "We can also get summary statistics about our data set using `info()` and `describe()`\n",
    "``` python\n",
    "data.info()\n",
    "data.describe()\n",
    "```\n",
    "\n",
    "\n",
    "Lets try this with our olympian dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medalist_wikidata_id</th>\n",
       "      <th>medalist_link</th>\n",
       "      <th>medalist_name</th>\n",
       "      <th>medal</th>\n",
       "      <th>delegation_wikidata_id</th>\n",
       "      <th>delegation_link</th>\n",
       "      <th>delegation_name</th>\n",
       "      <th>country_medal_wikidata_id</th>\n",
       "      <th>country_medal</th>\n",
       "      <th>country_medal_code2</th>\n",
       "      <th>...</th>\n",
       "      <th>nuts2_id</th>\n",
       "      <th>nuts2_name</th>\n",
       "      <th>nuts3_id</th>\n",
       "      <th>nuts3_name</th>\n",
       "      <th>nuts2_population</th>\n",
       "      <th>nuts3_population</th>\n",
       "      <th>nuts2_gdp</th>\n",
       "      <th>nuts3_gdp</th>\n",
       "      <th>nuts0_id</th>\n",
       "      <th>nuts0_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q11739253</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kim_Woo-jin_(arc...</td>\n",
       "      <td>Kim Woo-jin</td>\n",
       "      <td>gold</td>\n",
       "      <td>Q114753595</td>\n",
       "      <td>https://en.wikipedia.org/wiki/South_Korea_at_t...</td>\n",
       "      <td>South Korea at the 2024 Summer Olympics</td>\n",
       "      <td>Q884</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>KR</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1156472</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Brady_Ellison</td>\n",
       "      <td>Brady Ellison</td>\n",
       "      <td>silver</td>\n",
       "      <td>Q113581713</td>\n",
       "      <td>https://en.wikipedia.org/wiki/United_States_at...</td>\n",
       "      <td>United States at the 2024 Summer Olympics</td>\n",
       "      <td>Q30</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>US</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q18001487</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lee_Woo-seok</td>\n",
       "      <td>Lee Woo-seok</td>\n",
       "      <td>bronze</td>\n",
       "      <td>Q114753595</td>\n",
       "      <td>https://en.wikipedia.org/wiki/South_Korea_at_t...</td>\n",
       "      <td>South Korea at the 2024 Summer Olympics</td>\n",
       "      <td>Q884</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>KR</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q107619893</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kim_Je-deok</td>\n",
       "      <td>Kim Je-deok</td>\n",
       "      <td>gold</td>\n",
       "      <td>Q114753595</td>\n",
       "      <td>https://en.wikipedia.org/wiki/South_Korea_at_t...</td>\n",
       "      <td>South Korea at the 2024 Summer Olympics</td>\n",
       "      <td>Q884</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>KR</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q11739253</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kim_Woo-jin_(arc...</td>\n",
       "      <td>Kim Woo-jin</td>\n",
       "      <td>gold</td>\n",
       "      <td>Q114753595</td>\n",
       "      <td>https://en.wikipedia.org/wiki/South_Korea_at_t...</td>\n",
       "      <td>South Korea at the 2024 Summer Olympics</td>\n",
       "      <td>Q884</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>KR</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  medalist_wikidata_id                                      medalist_link  \\\n",
       "0            Q11739253  https://en.wikipedia.org/wiki/Kim_Woo-jin_(arc...   \n",
       "1             Q1156472        https://en.wikipedia.org/wiki/Brady_Ellison   \n",
       "2            Q18001487         https://en.wikipedia.org/wiki/Lee_Woo-seok   \n",
       "3           Q107619893          https://en.wikipedia.org/wiki/Kim_Je-deok   \n",
       "4            Q11739253  https://en.wikipedia.org/wiki/Kim_Woo-jin_(arc...   \n",
       "\n",
       "   medalist_name   medal delegation_wikidata_id  \\\n",
       "0    Kim Woo-jin    gold             Q114753595   \n",
       "1  Brady Ellison  silver             Q113581713   \n",
       "2   Lee Woo-seok  bronze             Q114753595   \n",
       "3    Kim Je-deok    gold             Q114753595   \n",
       "4    Kim Woo-jin    gold             Q114753595   \n",
       "\n",
       "                                     delegation_link  \\\n",
       "0  https://en.wikipedia.org/wiki/South_Korea_at_t...   \n",
       "1  https://en.wikipedia.org/wiki/United_States_at...   \n",
       "2  https://en.wikipedia.org/wiki/South_Korea_at_t...   \n",
       "3  https://en.wikipedia.org/wiki/South_Korea_at_t...   \n",
       "4  https://en.wikipedia.org/wiki/South_Korea_at_t...   \n",
       "\n",
       "                             delegation_name country_medal_wikidata_id  \\\n",
       "0    South Korea at the 2024 Summer Olympics                      Q884   \n",
       "1  United States at the 2024 Summer Olympics                       Q30   \n",
       "2    South Korea at the 2024 Summer Olympics                      Q884   \n",
       "3    South Korea at the 2024 Summer Olympics                      Q884   \n",
       "4    South Korea at the 2024 Summer Olympics                      Q884   \n",
       "\n",
       "              country_medal country_medal_code2  ... nuts2_id nuts2_name  \\\n",
       "0               South Korea                  KR  ...      NaN        NaN   \n",
       "1  United States of America                  US  ...      NaN        NaN   \n",
       "2               South Korea                  KR  ...      NaN        NaN   \n",
       "3               South Korea                  KR  ...      NaN        NaN   \n",
       "4               South Korea                  KR  ...      NaN        NaN   \n",
       "\n",
       "  nuts3_id nuts3_name nuts2_population nuts3_population nuts2_gdp nuts3_gdp  \\\n",
       "0      NaN        NaN              NaN              NaN       NaN       NaN   \n",
       "1      NaN        NaN              NaN              NaN       NaN       NaN   \n",
       "2      NaN        NaN              NaN              NaN       NaN       NaN   \n",
       "3      NaN        NaN              NaN              NaN       NaN       NaN   \n",
       "4      NaN        NaN              NaN              NaN       NaN       NaN   \n",
       "\n",
       "  nuts0_id  nuts0_name  \n",
       "0      NaN         NaN  \n",
       "1      NaN         NaN  \n",
       "2      NaN         NaN  \n",
       "3      NaN         NaN  \n",
       "4      NaN         NaN  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2202 entries, 0 to 2201\n",
      "Data columns (total 46 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   medalist_wikidata_id                   2202 non-null   object \n",
      " 1   medalist_link                          2202 non-null   object \n",
      " 2   medalist_name                          2202 non-null   object \n",
      " 3   medal                                  2202 non-null   object \n",
      " 4   delegation_wikidata_id                 2202 non-null   object \n",
      " 5   delegation_link                        2202 non-null   object \n",
      " 6   delegation_name                        2202 non-null   object \n",
      " 7   country_medal_wikidata_id              2195 non-null   object \n",
      " 8   country_medal                          2195 non-null   object \n",
      " 9   country_medal_code2                    2185 non-null   object \n",
      " 10  country_medal_code3                    2185 non-null   object \n",
      " 11  country_medal_ioc_country_code         2195 non-null   object \n",
      " 12  country_medal_NUTS_code                1000 non-null   object \n",
      " 13  date_of_birth                          2185 non-null   object \n",
      " 14  place_of_birth_wikidata_id             2108 non-null   object \n",
      " 15  place_of_birth                         2107 non-null   object \n",
      " 16  place_of_birth_located_in_wikidata_id  2069 non-null   object \n",
      " 17  place_of_birth_located_in              2068 non-null   object \n",
      " 18  place_of_birth_coordinates             2107 non-null   object \n",
      " 19  lat                                    2107 non-null   float64\n",
      " 20  lon                                    2107 non-null   float64\n",
      " 21  sex_or_gender_wikidata_id              2202 non-null   object \n",
      " 22  sex_or_gender                          2202 non-null   object \n",
      " 23  event_wikidata_id                      2202 non-null   object \n",
      " 24  event_link                             2202 non-null   object \n",
      " 25  event_name                             2202 non-null   object \n",
      " 26  event_part_of_wikidata_id              2182 non-null   object \n",
      " 27  event_part_of                          2182 non-null   object \n",
      " 28  event_sport_wikidata_id                1469 non-null   object \n",
      " 29  event_sport                            1469 non-null   object \n",
      " 30  event_part_of_sport_wikidata_id        2133 non-null   object \n",
      " 31  event_part_of_sport                    2133 non-null   object \n",
      " 32  sport_wikidata_id                      2182 non-null   object \n",
      " 33  sport                                  2182 non-null   object \n",
      " 34  nuts1_id                               958 non-null    object \n",
      " 35  nuts1_name                             958 non-null    object \n",
      " 36  nuts2_id                               958 non-null    object \n",
      " 37  nuts2_name                             958 non-null    object \n",
      " 38  nuts3_id                               958 non-null    object \n",
      " 39  nuts3_name                             958 non-null    object \n",
      " 40  nuts2_population                       938 non-null    float64\n",
      " 41  nuts3_population                       926 non-null    float64\n",
      " 42  nuts2_gdp                              790 non-null    float64\n",
      " 43  nuts3_gdp                              764 non-null    float64\n",
      " 44  nuts0_id                               958 non-null    object \n",
      " 45  nuts0_name                             958 non-null    object \n",
      "dtypes: float64(6), object(40)\n",
      "memory usage: 791.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>nuts2_population</th>\n",
       "      <th>nuts3_population</th>\n",
       "      <th>nuts2_gdp</th>\n",
       "      <th>nuts3_gdp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2107.000000</td>\n",
       "      <td>2107.000000</td>\n",
       "      <td>9.380000e+02</td>\n",
       "      <td>9.260000e+02</td>\n",
       "      <td>790.000000</td>\n",
       "      <td>764.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32.875258</td>\n",
       "      <td>15.408155</td>\n",
       "      <td>3.344670e+06</td>\n",
       "      <td>1.036040e+06</td>\n",
       "      <td>162627.032835</td>\n",
       "      <td>48659.216387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25.912377</td>\n",
       "      <td>74.974945</td>\n",
       "      <td>2.922280e+06</td>\n",
       "      <td>1.224460e+06</td>\n",
       "      <td>185225.606264</td>\n",
       "      <td>57416.854790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-46.429000</td>\n",
       "      <td>-157.857194</td>\n",
       "      <td>8.549300e+04</td>\n",
       "      <td>5.398100e+04</td>\n",
       "      <td>1756.750000</td>\n",
       "      <td>635.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31.287869</td>\n",
       "      <td>-6.266670</td>\n",
       "      <td>1.459701e+06</td>\n",
       "      <td>3.420415e+05</td>\n",
       "      <td>50711.405000</td>\n",
       "      <td>10489.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>41.561111</td>\n",
       "      <td>6.783333</td>\n",
       "      <td>2.320856e+06</td>\n",
       "      <td>6.428150e+05</td>\n",
       "      <td>107148.040000</td>\n",
       "      <td>24667.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>49.444444</td>\n",
       "      <td>44.514444</td>\n",
       "      <td>3.804906e+06</td>\n",
       "      <td>1.294767e+06</td>\n",
       "      <td>203606.360000</td>\n",
       "      <td>61278.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>68.438280</td>\n",
       "      <td>179.966667</td>\n",
       "      <td>1.238839e+07</td>\n",
       "      <td>6.871903e+06</td>\n",
       "      <td>782639.160000</td>\n",
       "      <td>252252.480000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lat          lon  nuts2_population  nuts3_population  \\\n",
       "count  2107.000000  2107.000000      9.380000e+02      9.260000e+02   \n",
       "mean     32.875258    15.408155      3.344670e+06      1.036040e+06   \n",
       "std      25.912377    74.974945      2.922280e+06      1.224460e+06   \n",
       "min     -46.429000  -157.857194      8.549300e+04      5.398100e+04   \n",
       "25%      31.287869    -6.266670      1.459701e+06      3.420415e+05   \n",
       "50%      41.561111     6.783333      2.320856e+06      6.428150e+05   \n",
       "75%      49.444444    44.514444      3.804906e+06      1.294767e+06   \n",
       "max      68.438280   179.966667      1.238839e+07      6.871903e+06   \n",
       "\n",
       "           nuts2_gdp      nuts3_gdp  \n",
       "count     790.000000     764.000000  \n",
       "mean   162627.032835   48659.216387  \n",
       "std    185225.606264   57416.854790  \n",
       "min      1756.750000     635.340000  \n",
       "25%     50711.405000   10489.000000  \n",
       "50%    107148.040000   24667.310000  \n",
       "75%    203606.360000   61278.950000  \n",
       "max    782639.160000  252252.480000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many of our datasets we will have missing values, one thing we need to figure out is where are these missing values, and what we should do with these missing values.\n",
    "To tabulate our missing values the following simple code can be used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medalist_wikidata_id                        0\n",
       "medalist_link                               0\n",
       "medalist_name                               0\n",
       "medal                                       0\n",
       "delegation_wikidata_id                      0\n",
       "delegation_link                             0\n",
       "delegation_name                             0\n",
       "country_medal_wikidata_id                   7\n",
       "country_medal                               7\n",
       "country_medal_code2                        17\n",
       "country_medal_code3                        17\n",
       "country_medal_ioc_country_code              7\n",
       "country_medal_NUTS_code                  1202\n",
       "date_of_birth                              17\n",
       "place_of_birth_wikidata_id                 94\n",
       "place_of_birth                             95\n",
       "place_of_birth_located_in_wikidata_id     133\n",
       "place_of_birth_located_in                 134\n",
       "place_of_birth_coordinates                 95\n",
       "lat                                        95\n",
       "lon                                        95\n",
       "sex_or_gender_wikidata_id                   0\n",
       "sex_or_gender                               0\n",
       "event_wikidata_id                           0\n",
       "event_link                                  0\n",
       "event_name                                  0\n",
       "event_part_of_wikidata_id                  20\n",
       "event_part_of                              20\n",
       "event_sport_wikidata_id                   733\n",
       "event_sport                               733\n",
       "event_part_of_sport_wikidata_id            69\n",
       "event_part_of_sport                        69\n",
       "sport_wikidata_id                          20\n",
       "sport                                      20\n",
       "nuts1_id                                 1244\n",
       "nuts1_name                               1244\n",
       "nuts2_id                                 1244\n",
       "nuts2_name                               1244\n",
       "nuts3_id                                 1244\n",
       "nuts3_name                               1244\n",
       "nuts2_population                         1264\n",
       "nuts3_population                         1276\n",
       "nuts2_gdp                                1412\n",
       "nuts3_gdp                                1438\n",
       "nuts0_id                                 1244\n",
       "nuts0_name                               1244\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dropping Null Values\n",
    "Now that we have the number of nulls there are various approaches we can take. The first of which is to just drop any row that has a null value. \n",
    "\n",
    "\n",
    "``` python \n",
    "no_nulls = data.dropna()\n",
    "```\n",
    "But imagine that we only care about certain rows not having null data we can limit our dropna statement using the `subset` parameter. \n",
    "Maybe we need date of birth and sport to be non null then we can do: \n",
    "\n",
    "``` python\n",
    "no_nulls_select = data.dropna(subset=[\"date_of_birth\", \"sport\"])\n",
    "```  \n",
    "Lets see the effect below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medalist_wikidata_id                     0\n",
       "medalist_link                            0\n",
       "medalist_name                            0\n",
       "medal                                    0\n",
       "delegation_wikidata_id                   0\n",
       "delegation_link                          0\n",
       "delegation_name                          0\n",
       "country_medal_wikidata_id                0\n",
       "country_medal                            0\n",
       "country_medal_code2                      0\n",
       "country_medal_code3                      0\n",
       "country_medal_ioc_country_code           0\n",
       "country_medal_NUTS_code                  0\n",
       "date_of_birth                            0\n",
       "place_of_birth_wikidata_id               0\n",
       "place_of_birth                           0\n",
       "place_of_birth_located_in_wikidata_id    0\n",
       "place_of_birth_located_in                0\n",
       "place_of_birth_coordinates               0\n",
       "lat                                      0\n",
       "lon                                      0\n",
       "sex_or_gender_wikidata_id                0\n",
       "sex_or_gender                            0\n",
       "event_wikidata_id                        0\n",
       "event_link                               0\n",
       "event_name                               0\n",
       "event_part_of_wikidata_id                0\n",
       "event_part_of                            0\n",
       "event_sport_wikidata_id                  0\n",
       "event_sport                              0\n",
       "event_part_of_sport_wikidata_id          0\n",
       "event_part_of_sport                      0\n",
       "sport_wikidata_id                        0\n",
       "sport                                    0\n",
       "nuts1_id                                 0\n",
       "nuts1_name                               0\n",
       "nuts2_id                                 0\n",
       "nuts2_name                               0\n",
       "nuts3_id                                 0\n",
       "nuts3_name                               0\n",
       "nuts2_population                         0\n",
       "nuts3_population                         0\n",
       "nuts2_gdp                                0\n",
       "nuts3_gdp                                0\n",
       "nuts0_id                                 0\n",
       "nuts0_name                               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_nulls=data.dropna()\n",
    "no_nulls.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medalist_wikidata_id                        0\n",
       "medalist_link                               0\n",
       "medalist_name                               0\n",
       "medal                                       0\n",
       "delegation_wikidata_id                      0\n",
       "delegation_link                             0\n",
       "delegation_name                             0\n",
       "country_medal_wikidata_id                   7\n",
       "country_medal                               7\n",
       "country_medal_code2                        17\n",
       "country_medal_code3                        17\n",
       "country_medal_ioc_country_code              7\n",
       "country_medal_NUTS_code                  1176\n",
       "date_of_birth                               0\n",
       "place_of_birth_wikidata_id                 85\n",
       "place_of_birth                             86\n",
       "place_of_birth_located_in_wikidata_id     123\n",
       "place_of_birth_located_in                 124\n",
       "place_of_birth_coordinates                 86\n",
       "lat                                        86\n",
       "lon                                        86\n",
       "sex_or_gender_wikidata_id                   0\n",
       "sex_or_gender                               0\n",
       "event_wikidata_id                           0\n",
       "event_link                                  0\n",
       "event_name                                  0\n",
       "event_part_of_wikidata_id                   0\n",
       "event_part_of                               0\n",
       "event_sport_wikidata_id                   708\n",
       "event_sport                               708\n",
       "event_part_of_sport_wikidata_id            49\n",
       "event_part_of_sport                        49\n",
       "sport_wikidata_id                           0\n",
       "sport                                       0\n",
       "nuts1_id                                 1215\n",
       "nuts1_name                               1215\n",
       "nuts2_id                                 1215\n",
       "nuts2_name                               1215\n",
       "nuts3_id                                 1215\n",
       "nuts3_name                               1215\n",
       "nuts2_population                         1235\n",
       "nuts3_population                         1246\n",
       "nuts2_gdp                                1382\n",
       "nuts3_gdp                                1408\n",
       "nuts0_id                                 1215\n",
       "nuts0_name                               1215\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "no_nulls_select = data.dropna(subset=[\"date_of_birth\",\"sport\"])\n",
    "no_nulls_select.isnull().sum()\n",
    "## as we can see we still have some null values but not in date of birth or sport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in Data\n",
    "\n",
    "Sometimes it makes sense to fill in the data there are two ways we could do this. For string variables we can just say if its null its unknown:  \n",
    "``` python\n",
    "data['categorical_column'].fillna('Unknown', inplace=True)\n",
    "```\n",
    "\n",
    "For numerical data we can fill in our unknown data with the column mean. However be careful when doing this there are many ways to fill in unknown data and they all have their pros and cons, in general it is safest to just drop any missing numerical values that you need. Feel free to reach out to a mentor for help deciding what is best.  \n",
    "``` python\n",
    "data['column_name'].fillna(data['column_name'].mean(), inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting Data Types \n",
    "Sometimes the wrong datatype can be assumed, for example if in a CSV, null integer values were reported as \"null\" or \"N/A\". We may end up reading in our data as a string, when we really wanted a numerical value. This is a problem because if we do any computations they will not be caried out correctly. 4+5 =9, but \"4\" + \"5\" is \"45\". In pandas we can use `.astype('dtype')`. So to turn a string into an int, we could drop our null values and do:  \n",
    "``` python\n",
    "data['intNotString'] = data['intNotString'].astype('int')\n",
    "```\n",
    "\n",
    "One other transformation we will often make is to turn our dates into datetimes, this makes it so that when you graph, Python treats the date as a date and not a string. To do this we can use `pd.to_datetime(data['date_column'])`.\n",
    "``` python\n",
    "data[date_column] = pd.to_datetime(data['date_column'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing Text Data \n",
    "Dealing with strings can be difficult in messy data. There are a couple of methods that can help with dealing with text data. Perhaps we think one column will be categorical but the data is messy. \"Group 1\", \"Group1\", \"group1\", and \"group 1\" maybe all refer to the same group. Two useful methods for strings that can help are the following. \n",
    "\n",
    "``` python\n",
    "data['string_column'] = data['string_column'].str.lower()\n",
    "data['string_column'] = data['string_column'].str.strip()\n",
    "```\n",
    "\n",
    "These commands turn all the letters to lowercase, and removes all spaces. Removing spaces can also be important. \"Group 1\" and \"Group 1 \" look the same in excel but are two different values when it comes to programatically dealing with data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monetary Adjustments \n",
    "\n",
    "#### Adjusting for Inflation\n",
    "\n",
    "When working with time-series data, that is data over time. Adjusting for inflation is important. USD in 2009 is not valued the same as USD in 2024. \n",
    "\n",
    "#### Standardizing to one Currency\n",
    "\n",
    "When dealing with data from multiple countries for example we may need to add an extra step. We should standardize the currencies to one currency. This is more complicated when dealing with multiple countries across multiple years. We will need to adjust for inflation for each individual country and then standardize to one currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country  year  adjusted_amount\n",
      "0     USA  2009      1500.000000\n",
      "1  Canada  2009      1418.181818\n",
      "2     USA  2024      1500.000000\n",
      "3  Canada  2024      1400.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'country': ['USA', 'Canada', 'USA', 'Canada'],\n",
    "    'year': [2009, 2009, 2024, 2024],\n",
    "    'amount': [1000, 1200, 1500, 1400]\n",
    "})\n",
    "\n",
    "# Adjusting for inflation (example adjustment factors)\n",
    "inflation_factors = pd.DataFrame({\n",
    "    'country': ['USA', 'Canada'],\n",
    "    'factor_2009': [1.0, 1.1],  # Example inflation factor for 2009\n",
    "    'factor_2024': [1.5, 1.3]   # Example inflation factor for 2024\n",
    "})\n",
    "\n",
    "# Merging datasets and adjusting amounts\n",
    "adjusted_data = data.merge(inflation_factors, on='country')\n",
    "adjusted_data['adjusted_amount'] = adjusted_data.apply(\n",
    "    lambda row: row['amount'] * row['factor_2024'] / row['factor_2009'] if row['year'] == 2009 else row['amount'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "adjusted_data = adjusted_data[['country', 'year', 'adjusted_amount']]\n",
    "print(adjusted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting for Population \n",
    "\n",
    "When dealing with larger scale collected data we often need to take population size into account. If we have covid infections, we want to scale that by the population within our geographic region of covid infections. To do this we can simply standardize over our population: \n",
    "$$ \\frac{\\text{covid infections}_i}{\\text{total population}_i}$$\n",
    "Where i is the location, so for each location our output would be the number of covid infection in that location divided by the number of people in the location\n",
    "\n",
    "For this we could make a new column:\n",
    "``` python\n",
    "data[\"covidPerCapita\"] = data[\"covid infections\"] / data[\"total population\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization\n",
    "\n",
    "If we have data that is not necessarily related to other columns but we want to understand scale properly we can use standardization\n",
    "\n",
    "The simplest definition of standardization is: \n",
    "$$\\phi(x) = \\frac{x - \\overline{x}}{\\sigma}$$\n",
    "Where $\\overline{x}$ is the column mean, and $\\sigma$ is the standard deviation of the column\n",
    "\n",
    "In pandas we do the following:\n",
    "``` python\n",
    "data[\"standardized_col\"] = (data[\"col\"] - data[\"col\"].mean()) / data[\"col\"].std()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Joining Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to merge or join datasets. To do this we will do what is called a join. There are 4 main types of joins to know **left**, **right**, **inner**, and **outer**. There are other variations of joins but these 4 are all that you need to understand to do most, if not all dataset joining. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the difference between left, right, inner and outer joins?\n",
    "\n",
    "The main difference between these joins is how we connect the data, and how we deal with misaligned data, that is data that appears in one dataset but not the other.\n",
    "\n",
    "#### In all joins we need a join key (what column we are joining on)\n",
    "\n",
    "Our join key is what we use to match rows in each dataset to eachother. For example if I have a dataset with geocodes and wanted to combine two datasets at the same geographic level with the same geocode format I could use a join to make a new dataset containing all the columns from the two datasets I am combining. \n",
    "\n",
    "In pandas, we use the following syntax:\n",
    "``` pd.merge(how=\"method (left,right,inner,outer)\", on=[join_key])```\n",
    "\n",
    "### Left and Right Joins \n",
    "\n",
    "Left and right joins are very simple to think about. If we think about our first dataset as the left dataset, and the second datasets as the right dataset, the left or right join simply indicates which dataset to keep intact. \n",
    "\n",
    "For a left join any record (or row) that exists in the left dataset will exist in the final dataset regardless of whether there is data in the right dataset corresponding to the joinkey of the record in the left dataset. In this case null values will be used to fill in the values of the right data columns \n",
    "\n",
    "The right join is the opposite all rows from the right dataset will be stored in the new joined dataset and any data in the right dataset that does not have matching data in the left dataset will be filled with null values. \n",
    "\n",
    "*Important Note:* If you use a left join and data in the right side does not exist on the left dataset then it will be dropped and vice versa. If keeping all the data is important Outer Joins are a better choice of join method.\n",
    "\n",
    "*Important Note:* A futher implementation detail that is important to remember is that if you use a left join for example. If a record in the left dataset has multiple records in the right dataset then multiple rows will be created as the left will \"join\" to both matching records on the right side. This applies to right joins as well but vice-versa\n",
    "\n",
    "```python\n",
    "## left join \n",
    "pd.merge(left, right, how=\"left\", on=\"joinkey\")\n",
    "## right join\n",
    "pd.merge(left, right, how='right', on=\"joinkey\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner and Outer Joins\n",
    "\n",
    "#### Inner Joins\n",
    "\n",
    "An inner join connects the two datasets for all records that have data within both datasets. So any data that is only in the right, or only in the left are dropped. \n",
    "\n",
    "\n",
    "#### Outer Joins\n",
    "\n",
    "Outer joins do the opposite they combine two datasets in their entirety filling in any missing data with null values. So all records in both left and right dataset will continue to exist in the joined dataset, that is with null values for any mismatches.\n",
    "\n",
    "```python\n",
    "## inner join \n",
    "pd.merge(left, right, how=\"inner\", on=\"joinkey\")\n",
    "## outer join\n",
    "pd.merge(left, right, how='outer', on=\"joinkey\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating Datasets \n",
    "\n",
    "Often times our data could be split into multiple files. For example they may pe partitioned by year. If they have the same columns you can connect them into one DataFrame. For example: \n",
    "``` python \n",
    "data2021 = pd.load_csv(\"data2021.csv\")\n",
    "data2022 = pd.load_csv(\"data2022.csv\")\n",
    "data2023 = pd.load_csv(\"data2023.csv\")\n",
    "all_data = pd.concat([data2021, data2022, data2023])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "\n",
    "Sometimes our data is too granular, or we need to aggregate by some metric. For example if we have daily data we may want to aggregate to the year or to the month. Pandas gives us the ability to group by a column, and then aggregate using one of two methods, either one aggregation method for all columns (e.g., mean, median, sum). Or you can build a dictionary of columns to aggregation function. An example of both are shown below for say aggregating down and grouping by month. \n",
    "\n",
    "``` python\n",
    "data = data.groupby('Month').agg('mean').replace_index()\n",
    "## Alternatively:\n",
    "data = data.groupby('Month').agg({\n",
    "    'col1': 'mean',\n",
    "    'col2': 'sum',\n",
    "    'col3': 'median'\n",
    "    ...\n",
    "})\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
